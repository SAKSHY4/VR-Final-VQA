{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11861298,"sourceType":"datasetVersion","datasetId":7427077}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \\\n  datasets \\\n  sentencepiece \\\n  matplotlib \\\n  rouge_score \\\n  evaluate \\\n  nltk \\\n  transformers \\\n  bitsandbytes \\\n  accelerate \\\n  trl \\\n  peft \\\n  torch torchvision \\\n  pillow \\\n  tqdm \\\n  scikit-learn \\\n  bert-score \\\n  flash-attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:44:12.292648Z","iopub.execute_input":"2025-05-18T16:44:12.292837Z","iopub.status.idle":"2025-05-18T16:45:48.399230Z","shell.execute_reply.started":"2025-05-18T16:44:12.292821Z","shell.execute_reply":"2025-05-18T16:45:48.398239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mv /kaggle/input/vqa-data-for-model-training/checkpoint-3500 /kaggle/working/qlora_gemma2b_vqa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:46:54.189054Z","iopub.execute_input":"2025-05-18T16:46:54.189891Z","iopub.status.idle":"2025-05-18T16:46:58.575223Z","shell.execute_reply.started":"2025-05-18T16:46:54.189853Z","shell.execute_reply":"2025-05-18T16:46:58.574272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Force single-GPU usage\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync,expandable_segments:True\"\nimport json\nimport time\nimport random\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\n\n# HuggingFace imports\nfrom transformers import (\n    AutoProcessor, \n    BitsAndBytesConfig,\n    TrainingArguments,\n    LlavaForConditionalGeneration,\n    LlavaProcessor,\n    AutoTokenizer\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom trl import SFTTrainer\n\n# For visualization\nimport matplotlib.pyplot as plt\n\n# Set seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\ntorch.backends.cuda.matmul.allow_tf32 = False  # T4 doesn't support TF32\ntorch.backends.cudnn.allow_tf32 = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:47:05.220069Z","iopub.execute_input":"2025-05-18T16:47:05.220393Z","iopub.status.idle":"2025-05-18T16:47:31.621174Z","shell.execute_reply.started":"2025-05-18T16:47:05.220365Z","shell.execute_reply":"2025-05-18T16:47:31.620635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure QLoRA (4-bit Quantized Low-Rank Adaptation)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",      # NormalFloat4 quantization\n    bnb_4bit_use_double_quant=True, # Nested quantization for 0.4 more bits saved\n    bnb_4bit_compute_dtype=torch.float16,\n    quantize_kv_cache=False          # Quantize key/value cache for memory efficiency\n)\n\n# Load LLaVA-Gemma-2B model and processor\ncheckpoint = \"Intel/llava-gemma-2b\"\n\n# 1. Load tokenizer FIRST\ntokenizer = AutoTokenizer.from_pretrained(\n    checkpoint,\n    trust_remote_code=True,\n    use_fast=False\n)\n\n# 2. Create processor with explicit tokenizer\nprocessor = LlavaProcessor.from_pretrained(\n    checkpoint,\n    tokenizer=tokenizer,  # ← Critical!\n    trust_remote_code=True,\n    use_fast=False\n)\n\n# 3. Set required attributes from model config\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    checkpoint,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,\n    use_flash_attention_2=True\n)\n\nprocessor.patch_size = model.config.vision_config.patch_size  # 14 for CLIP\nprocessor.num_additional_image_tokens = 1\nprocessor.vision_feature_select_strategy = \"default\"\n\n# 4. Verify tokenizer exists\nprint(f\"Tokenizer class: {type(processor.tokenizer).__name__}\")  # Now shows LlamaTokenizerFast\n\n# Enable KV-cache for faster inference\nmodel.config.use_cache = False\n\n# Print model size info\nprint(f\"Model loaded with {model.num_parameters() / 1e9:.2f}B parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:50:11.985987Z","iopub.execute_input":"2025-05-18T16:50:11.987098Z","iopub.status.idle":"2025-05-18T16:52:15.974293Z","shell.execute_reply.started":"2025-05-18T16:50:11.987070Z","shell.execute_reply":"2025-05-18T16:52:15.973598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure LoRA \nlora_config = LoraConfig(\n    r=16,                          # Rank of update matrices (higher = more capacity, more memory)\n    lora_alpha=32,                 # Scaling factor (typically 2*r)\n    lora_dropout=0.1,              # Dropout probability for robustness\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention modules\n        \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP modules\n    ],\n    bias=\"none\",                   # Don't add trainable bias\n    task_type=\"CAUSAL_LM\"          # Task type for LLaVA-Gemma model\n)\n\n# Prepare model for QLoRA training\nprint(\"Applying LoRA adapters...\")\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Should show <1% of parameters being trained","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:15.975420Z","iopub.execute_input":"2025-05-18T16:52:15.976019Z","iopub.status.idle":"2025-05-18T16:52:16.429437Z","shell.execute_reply.started":"2025-05-18T16:52:15.976000Z","shell.execute_reply":"2025-05-18T16:52:16.428844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset as TorchDataset\n\nclass VQADataset(TorchDataset):\n    \"\"\"Custom VQA dataset that handles multiple JSON formats\"\"\"\n    \n    def __init__(self, json_path, img_dir, processor, max_samples=None):\n        self.processor = processor\n        self.img_dir = img_dir             # ← store the image folder here\n        \n        # Load data\n        print(f\"Loading data from {json_path}...\")\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        \n        self.samples = []\n        \n        if isinstance(data, list):\n            print(f\"Detected list format with {len(data)} examples\")\n            for item in data:\n                if self._validate_item(item):\n                    self.samples.append(self._process_item(item))\n        \n        elif isinstance(data, dict):\n            print(f\"Detected dictionary format with {len(data)} images\")\n            for img_id, entry in data.items():\n                if not isinstance(entry, dict) or \"questions\" not in entry:\n                    continue\n                \n                # Build full path from the folder you passed in\n                image_path = os.path.join(self.img_dir, img_id)\n                if not os.path.exists(image_path):\n                    # Try common extensions\n                    for ext in ['.jpg', '.jpeg', '.png']:\n                        alt_path = os.path.join(self.img_dir, img_id + ext)\n                        if os.path.exists(alt_path):\n                            image_path = alt_path\n                            break\n                    else:  # No valid path found\n                        continue\n\n                \n                for q in entry[\"questions\"]:\n                    if not all(k in q for k in (\"question\",\"options\",\"answer\")):\n                        continue\n                    self.samples.append({\n                        \"image_path\": image_path,\n                        \"question\":    q[\"question\"],\n                        \"options\":     [str(o) for o in q[\"options\"]],\n                        \"answer\":      str(q[\"answer\"]).lower()\n                    })\n        \n        # Limit dataset size if requested\n        if max_samples and max_samples < len(self.samples):\n            self.samples = random.sample(self.samples, max_samples)\n            \n        print(f\"Loaded {len(self.samples)} VQA samples\")\n\n        # After creating samples\n        print(f\"First few image paths:\")\n        for i in range(min(3, len(self.samples))):\n            print(f\"- {self.samples[i]['image_path']}\")\n\n        \n        # Show a few examples\n        if len(self.samples) > 0:\n            print(\"\\nExample samples:\")\n            for i in range(min(2, len(self.samples))):\n                print(f\"Question: {self.samples[i]['question']}\")\n                print(f\"Options: {', '.join(self.samples[i]['options'])}\")\n                print(f\"Answer: {self.samples[i]['answer']}\")\n                print()\n    \n    def _validate_item(self, item):\n        \"\"\"Check if an item has all required fields\"\"\"\n        required = [\"image_path\", \"question\", \"options\", \"answer\"]\n        if all(k in item for k in required):\n            if os.path.exists(item[\"image_path\"]):\n                return True\n        return False\n    \n    def _validate_question(self, question):\n        \"\"\"Check if a question entry has all required fields\"\"\"\n        required = [\"question\", \"options\", \"answer\"]\n        return all(k in question for k in required)\n    \n    def _process_item(self, item):\n        \"\"\"Process a single item into the required format\"\"\"\n        return {\n            \"image_path\": item[\"image_path\"],\n            \"question\": item[\"question\"],\n            \"options\": [str(opt) for opt in item[\"options\"]],\n            \"answer\": str(item[\"answer\"]).lower()\n        }\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.430212Z","iopub.execute_input":"2025-05-18T16:52:16.430494Z","iopub.status.idle":"2025-05-18T16:52:16.442772Z","shell.execute_reply.started":"2025-05-18T16:52:16.430453Z","shell.execute_reply":"2025-05-18T16:52:16.442110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch, processor, device=None):\n    \"\"\"Process a batch of examples for training\"\"\"\n    if not hasattr(processor, 'patch_size') or processor.patch_size is None:\n        processor.patch_size = 14\n    \n    if not hasattr(processor, 'num_additional_image_tokens') or processor.num_additional_image_tokens is None:\n        processor.num_additional_image_tokens = 1\n\n    images = []\n    texts = []\n    answers = []\n    \n    for example in batch:\n        try:\n            # Load and convert image\n            image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n            \n            # Format prompt with options\n            options_str = \", \".join(example[\"options\"])\n            prompt = f\"<image>\\n{example['question']}\\nOptions: {options_str}\\nAnswer with just one option.\"\n            \n            # Create chat message\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            formatted_prompt = processor.tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            \n            images.append(image)\n            texts.append(formatted_prompt)\n            answers.append(example[\"answer\"])\n            \n        except Exception as e:\n            print(f\"Error processing example: {e}\")\n            continue\n    \n    if len(images) == 0:\n        raise ValueError(\"No valid examples in batch\")\n    \n    # Process inputs\n    inputs = processor(\n    text=texts,\n    images=images,\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True\n)\n    \n    # Create labels\n    labels = inputs[\"input_ids\"].clone()\n    \n    # Mask padding tokens\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    \n    # For each example, mask the input portion (we only want loss on the answer tokens)\n    for i, answer in enumerate(answers):\n        # Get answer length in tokens\n        answer_tokens = processor.tokenizer.encode(answer, add_special_tokens=False)\n        answer_len = len(answer_tokens)\n        \n        # Get sequence length for this example (excluding padding)\n        seq_len = inputs[\"attention_mask\"][i].sum().item()\n        \n        # Mask everything except the last few tokens which should include the answer\n        # This is an approximation - in production you might need more precise masking\n        labels[i, :seq_len-answer_len] = -100\n    \n    inputs[\"labels\"] = labels\n    \n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.444275Z","iopub.execute_input":"2025-05-18T16:52:16.444525Z","iopub.status.idle":"2025-05-18T16:52:16.467410Z","shell.execute_reply.started":"2025-05-18T16:52:16.444507Z","shell.execute_reply":"2025-05-18T16:52:16.466635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nfrom transformers import TrainerCallback\n\n# 1. Compute total update steps\ndef get_total_updates(dataset_len, batch_size, accum_steps, epochs):\n    updates_per_epoch = math.ceil(dataset_len / (batch_size * accum_steps))\n    return updates_per_epoch * epochs\n\n# 2. Define a callback to print progress\nclass StepProgressCallback(TrainerCallback):\n    def on_train_begin(self, args, state, control, **kwargs):\n        total = state.max_steps\n        print(\n            f\"⏳ Starting training: \"\n            f\"{total} total update steps \"\n            f\"({args.num_train_epochs} epochs, \"\n            f\"batch_size={args.per_device_train_batch_size}, \"\n            f\"grad_accum={args.gradient_accumulation_steps})\"\n        )\n        return control\n\n    def on_step_end(self, args, state, control, **kwargs):\n        # Log every logging_steps update\n        if state.global_step % args.logging_steps == 0:\n            print(f\"Step {state.global_step}/{state.max_steps}\")\n        return control","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.468092Z","iopub.execute_input":"2025-05-18T16:52:16.468348Z","iopub.status.idle":"2025-05-18T16:52:16.488857Z","shell.execute_reply.started":"2025-05-18T16:52:16.468330Z","shell.execute_reply":"2025-05-18T16:52:16.488139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport os\n\nclass AdapterCheckpointCallback(TrainerCallback):\n    def on_save(self, args, state, control, **kwargs):\n        # Folder where the Trainer just saved its checkpoint\n        ckpt_folder = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n        # Path to save only the LoRA adapter weights\n        adapter_path = os.path.join(ckpt_folder, \"adapter_model\")\n        # Save only the low-rank adapter tensors & config\n        kwargs[\"model\"].save_pretrained(adapter_path, safe_serialization=True)\n        # (Optional) remove full-model bin to free space\n        full_model = os.path.join(ckpt_folder, \"pytorch_model.bin\")\n        if os.path.exists(full_model):\n            os.remove(full_model)\n        return control","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.489654Z","iopub.execute_input":"2025-05-18T16:52:16.489860Z","iopub.status.idle":"2025-05-18T16:52:16.506804Z","shell.execute_reply.started":"2025-05-18T16:52:16.489838Z","shell.execute_reply":"2025-05-18T16:52:16.506094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTConfig\nimg_dir=\"/kaggle/input/vqa-data-for-model-training/tinygptv_images/kaggle/working/tinygptv_images\"\n\ndef train_model(\n    model,\n    processor,\n    train_json,\n    val_json=None,\n    output_dir=\"./vqa_model_output\",\n    epochs=3,\n    batch_size=2,\n    grad_accum_steps=4,\n    learning_rate=3e-4,\n    max_samples=None,\n):\n    \"\"\"Complete training function with appropriate configurations\"\"\"\n    \n    # Create training dataset\n    print(\"Creating training dataset...\")\n    train_dataset = VQADataset(train_json, img_dir, processor, max_samples)\n    \n    # Create validation dataset if provided\n    val_dataset = None\n    if val_json and os.path.exists(val_json):\n        print(\"Creating validation dataset...\")\n        val_dataset = VQADataset(val_json, img_dir, processor, 800)\n    \n    # Configure training arguments with appropriate parameters for QLoRA\n    training_args = SFTConfig(\n         output_dir=output_dir,\n         num_train_epochs=epochs,\n         per_device_train_batch_size=batch_size, \n         per_device_eval_batch_size=batch_size,  # ← Critical! Default is same as train batch size\n         gradient_checkpointing=True,    # ← Enable for both train/eval\n         gradient_checkpointing_kwargs={\"use_reentrant\": False},\n         gradient_accumulation_steps=grad_accum_steps,\n         learning_rate=learning_rate,\n         report_to=None,\n         lr_scheduler_type=\"cosine\",\n         warmup_ratio=0.03,\n         weight_decay=0.01,\n         tf32=False,\n         fp16=True,\n         logging_steps=1,\n         disable_tqdm=False,        \n         save_strategy=\"steps\",         # or \"epoch\"\n         save_steps=500,                # save every 500 optimization steps\n         save_total_limit=1,            # keep only the 3 most recent checkpoints\n         eval_strategy=\"steps\" if val_dataset else \"no\",\n         eval_steps=500,\n         run_name=f\"vqa-training-{int(time.time())}\",\n         remove_unused_columns=False,\n         push_to_hub=False,\n         dataloader_pin_memory=True,\n         dataloader_num_workers=8,\n         max_grad_norm=0.3,\n         optim=\"paged_adamw_8bit\",\n         dataset_kwargs={\"skip_prepare_dataset\": True}\n     )\n    \n    # Define custom collation function\n    def collator(examples):\n        return collate_fn(examples, processor, device=model.device)\n    \n    # Initialize trainer\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=collator,\n        processing_class=processor.tokenizer,\n        callbacks=[AdapterCheckpointCallback, StepProgressCallback]\n        #compute_metrics=compute_metrics,  # Add metrics computation after first successful run\n    )\n    \n    # Enable training optimizations\n    model.train()\n    model.gradient_checkpointing_enable()\n    \n    # Run training\n    print(\"Starting training...\")\n    trainer.train(resume_from_checkpoint=True)\n    \n    # Save the model (adapter weights)\n    print(\"Saving LoRA adapter to\", output_dir)\n    model.save_pretrained(output_dir, safe_serialization=True)\n    print(f\"LoRA adapter saved to {output_dir}\")\n   \n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.507723Z","iopub.execute_input":"2025-05-18T16:52:16.507922Z","iopub.status.idle":"2025-05-18T16:52:16.523738Z","shell.execute_reply.started":"2025-05-18T16:52:16.507908Z","shell.execute_reply":"2025-05-18T16:52:16.523029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run training with small batch to validate pipeline\nmodel, processor = train_model(\n    model=model,\n    processor=processor,\n    train_json=\"/kaggle/input/vqa-data-for-model-training/Segregated Data/Training Data/vqa_outputs_fixed.json\",\n#    val_json=\"/kaggle/input/vqa-data-for-model-training/Segregated Data/Validation Data/vqa_outputs_fixed.json\",\n    output_dir=\"/kaggle/working/qlora_gemma2b_vqa\",\n    epochs=1,\n    batch_size=3,\n    grad_accum_steps=4\n)\n\n# Once pipeline works, remove the max_samples limit for full training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:52:16.524517Z","iopub.execute_input":"2025-05-18T16:52:16.524729Z","execution_failed":"2025-05-18T16:54:33.672Z"}},"outputs":[],"execution_count":null}]}